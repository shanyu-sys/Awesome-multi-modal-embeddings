# Awesome-multi-modal-embeddings
This list includes the most state-of-the-art projects on multi-modal-embeddings, which are
* with high impact and code stability
  * high number of GitHub stars
  * model used in other papers' evaluations
* paper accepted in top conferences (CVPR, ICML, .etc)

Note that some of projects with more than 3 modalities may not be widely used.

## More than 3 modalities

* [ImageBind: One Embedding Space To Bind Them All](./project-details/ImageBind.md)
* [High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning](./project-details/HighMMT.md)


## 3 Modalities: vision-language-audio

## 2 Modalities: vision-language

  * [LAVIS - A Library for Language-Vision Intelligence](./project-details/lavis.md)
  * [OpenCLIP - An open source implementation of OpenAI's CLIP](./project-details/OpenCLIP.md)

  * [(BEiT-3) Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks](./project-details/beit3.md)



#### Paper

* [Link]()
* Conference: 
* Year: 2023
* tasks

#### Code

* [Github]()
* Stars: **k**
* license: 

#### Installation and get embeddings

##### Installation: 

`pip install .`

* dep: torch 1.13+ ; python 3.8

##### get embeddings:

```python
```

